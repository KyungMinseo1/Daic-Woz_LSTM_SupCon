import sys
from pathlib import Path
project_root = Path().cwd().resolve().parent
sys.path.insert(0, str(project_root))

import sqlite3

import os, argparse, path_config, shutil
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
from loguru import logger

import torch
from torch_geometric.utils import to_networkx, k_hop_subgraph
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
import torch.nn.functional as F
from torch_geometric.explain import Explainer, GNNExplainer
from captum.attr import IntegratedGradients
from tqdm import tqdm
import time

from graph._multimodal_model_bilstm.GAT_explanation import GATJKClassifier as BiLSTMV2GAT
from graph.multimodal_topic_bilstm_proxy.dataset_explanation import make_graph as TopicProxyBiLSTM_make_graph

plt.rcParams['font.family'] ='Malgun Gothic'
plt.rcParams['axes.unicode_minus'] =False

logger.remove()
logger.add(
  sys.stdout,
  colorize=True,
  format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
)

V2_MODEL = {
  'multimodal_topic_bilstm_proxy':BiLSTMV2GAT
}

MAKE_GRAPH = {
  'multimodal_topic_bilstm_proxy':TopicProxyBiLSTM_make_graph
}

def fetch_from_db(db_path):
  con = sqlite3.connect(db_path)
  cursor = con.cursor()
  cursor.execute('''
    SELECT param_name, param_value 
    FROM trial_params
    WHERE trial_id = (
      SELECT trial_id
      FROM trial_values
      ORDER BY value DESC
      LIMIT 1
    );
  ''')
  best_hyperparams_list = cursor.fetchall()
  best_hyperparams_dict = {}

  for k, v in best_hyperparams_list:
    if k not in ['batch_size', 'focal_alpha', 'focal_gamma', 'lr', 'optimizer', 'weight_decay']:
      if k in ['use_text_proj', 'use_attention']:
        best_hyperparams_dict[k] = True if v==0.0 else False
      elif k in ['num_layers', 'bilstm_num_layers']:
        best_hyperparams_dict[k] = int(v)
      else:
        best_hyperparams_dict[k] = v

  cursor.execute('''
    SELECT value
    FROM trial_values
    ORDER BY value DESC
    LIMIT 1
  ''')
  best_f1 = cursor.fetchone()[0]
  
  return best_hyperparams_dict, best_f1

model_dir = 'checkpoints_optuna'
model_dir_ = 'multimodal_topic_bilstm_proxy_v2'
save_dir = 'graph_visualization'
save_dir_ = 'multimodal_topic_bilstm_proxy_v2_id_405_ipynb'
id = 405
mode = 'multimodal_topic_bilstm_proxy'
version = 2

best_model_path = os.path.join(path_config.ROOT_DIR, model_dir, model_dir_, 'best_model.pth')
db_path = os.path.join(path_config.ROOT_DIR, model_dir, model_dir_, 'logs', 'optuna_study.db')
assert os.path.exists(best_model_path) and os.path.exists(db_path), logger.error("Model path is wrong. Try again.")

logger.info(f"Processing data (Mode: {mode}, Id: {id})")

if "multimodal" in mode:
  logger.info(f"Doing with multimodal mode")
  graphs, dim_list, extras = MAKE_GRAPH[mode](
    ids = [id],
    labels = [1],                   # Temporary Label
    model_name = 'sentence-transformers/all-MiniLM-L6-v2',
    use_summary_node = True,
    t_t_connect = False,
    v_a_connect = False,
    explanation = True
  )

  t_dim = dim_list[0]
  v_dim = dim_list[1]
  a_dim = dim_list[2]

else:
  logger.info(f"Doing with non-multimodal mode")
  graphs, dim_list, extras = MAKE_GRAPH[mode](
    ids = [id],
    labels = 1,                   # Temporary Label
    model_name = 'sentence-transformers/all-MiniLM-L6-v2',
    use_summary_node = True,
    t_t_connect = False,
    explanation = True
  )

  t_dim = dim_list[0]
  if 'bimodal' in mode:
    v_dim = dim_list[1]

topic_node_id, utterances, vision_input, audio_input = extras

best_hyperparams_dict, best_f1 = fetch_from_db(db_path)

logger.info(f"Best Params")
for k, v in best_hyperparams_dict.items():
  logger.info(f"  - {k}: {v}")
logger.info(f"=> F1-score: {best_f1}")

logger.info("==============================")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger.info(f"Loading your model (Device: {device})")

assert version in [1,2], logger.error("Version should be int type 1 or 2")

if version == 2:
  model_dict = V2_MODEL

dropout_dict = {
  'text_dropout':best_hyperparams_dict.get('t_dropout', 0.0),
  'graph_dropout':best_hyperparams_dict.get('g_dropout', 0.0),
  'vision_dropout':best_hyperparams_dict.get('v_dropout', 0.0),
  'audio_dropout':best_hyperparams_dict.get('a_dropout', 0.0)
}


model = model_dict[mode](
  text_dim=t_dim,
  vision_dim=v_dim,
  audio_dim=a_dim,
  hidden_channels=256 if best_hyperparams_dict['use_text_proj'] else t_dim,
  num_layers=best_hyperparams_dict['num_layers'],
  bilstm_num_layers=best_hyperparams_dict['bilstm_num_layers'],
  num_classes=2,
  dropout_dict=dropout_dict,
  heads=8,
  use_attention=best_hyperparams_dict['use_attention'],
  use_summary_node=True,
  use_text_proj=best_hyperparams_dict['use_text_proj']
).to(device)

best_model_state_dict = torch.load(best_model_path)
model.load_state_dict(best_model_state_dict)

sample_loader = DataLoader(graphs)

model.eval()
with torch.no_grad():
  for data in sample_loader:
    data = data.to(device)
    result, x, flat_node_types = model(data, explanation=True)
    x = x.cpu()

topic_indices = [i for i, v in enumerate(graphs[0].node_types) if v == 'topic']
text_indices = [i for i, v in enumerate(graphs[0].node_types) if v == 'transcription']
proxy_indices = [i for i, v in enumerate(graphs[0].node_types) if v == 'proxy']
vision_indices = [i for i, v in enumerate(graphs[0].node_types) if v == 'vision']
audio_indices = [i for i, v in enumerate(graphs[0].node_types) if v == 'audio']

source_indices = graphs[0].edge_index[0].numpy()
target_indices = graphs[0].edge_index[1].numpy()

utterances, vision_input, audio_input = np.array(utterances), np.array(vision_input), np.array(audio_input)

# For the fist topic
topic_target_indices = np.where(target_indices==topic_indices[1])                         # extract index of target edge_index where target is the certain topic node
text_source_ids = source_indices[topic_target_indices]                                    # extract text node ids from source edge_index
text_valid_ids = text_source_ids[text_source_ids>len(topic_indices)]                      # delete topic node ids
topic_text_indices = np.where(np.isin(text_indices, text_valid_ids)==True)                # extract text(utterance) index from text indices
topic_utterances = utterances[topic_text_indices]

text_target_indices = np.where(np.isin(target_indices, text_valid_ids)==True)             # extract index of target edge_index where target is the text from first topic node
proxy_source_ids = source_indices[text_target_indices]                                    # extract proxy node ids from source edge_index

proxy_target_indices = np.where(np.isin(target_indices, proxy_source_ids)==True)          # extract index of target edge_index where target is the proxy from text
vision_audio_source_ids = source_indices[proxy_target_indices]                            # extract vision/audio node ids from source edge_index
topic_vision_indices = np.where(np.isin(vision_indices, vision_audio_source_ids)==True)   # extract vision index from vision indices
topic_audio_indices = np.where(np.isin(audio_indices, vision_audio_source_ids)==True)     # extract audio index from vision indices
topic_vision = vision_input[topic_vision_indices] 
topic_audio = vision_input[topic_audio_indices]

topic_node_dict = {v+1:str(k) for k,v in topic_node_id.items()}

class ProgressGNNExplainer(GNNExplainer):
    """ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í•˜ê³ , ë§ˆìŠ¤í¬ ì´ˆê¸°í™” ì•ˆì „ì¥ì¹˜ë¥¼ í¬í•¨í•œ GNN Explainer"""
    def __init__(self, epochs=100, **kwargs):
        super().__init__(epochs=epochs, **kwargs)
        self.loss_history = []
    
    def _train(self, model, x, edge_index, *, target, index, **kwargs):
        """í•™ìŠµ ê³¼ì •ì—ì„œ ì§„í–‰ ìƒí™© ì¶œë ¥ ë° ë§ˆìŠ¤í¬ ê°•ì œ ì´ˆê¸°í™”"""
        
        # [ì•ˆì „ì¥ì¹˜] ë§ˆìŠ¤í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ì„¤ì • ê°•ì œ ì£¼ì… ë° ì´ˆê¸°í™”
        if self.node_mask is None and self.edge_mask is None:
            # print("âš ï¸ Masks not found. Forcing initialization...")
            if self.explainer_config.node_mask_type is None:
                self.explainer_config.node_mask_type = 'attributes' # ê¸°ë³¸ê°’: ë…¸ë“œ ì†ì„± ë§ˆìŠ¤í‚¹ # type: ignore
            if self.explainer_config.edge_mask_type is None:
                self.explainer_config.edge_mask_type = 'object'     # ê¸°ë³¸ê°’: ì—£ì§€ ìœ ë¬´ ë§ˆìŠ¤í‚¹ # type: ignore
                
            self._initialize_masks(x, edge_index)
            # print("âœ… Masks initialized manually.")

        # Optimizer ì´ˆê¸°í™” (ë§ˆìŠ¤í¬ê°€ ìƒì„±ëœ í›„ ì‹¤í–‰)
        parameters = []
        if self.node_mask is not None:
            parameters.append(self.node_mask)
        if self.edge_mask is not None:
            parameters.append(self.edge_mask)
            
        if len(parameters) == 0:
             # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì§„ì§œ ì˜¤ë¥˜
            raise ValueError("No masks to optimize! Check Explainer config.")

        self.optimizer = torch.optim.Adam(parameters, lr=self.lr)
        
        # í•™ìŠµ ë£¨í”„
        pbar = tqdm(range(self.epochs), desc="Training GNN Explainer")
        for epoch in pbar:
            self.optimizer.zero_grad()
            h = model(x, edge_index, **kwargs)
            loss = self._loss(h, target)
            loss.backward()
            self.optimizer.step()
            
            self.loss_history.append(loss.item())
            if (epoch + 1) % 20 == 0:
                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})


class ModelWrapper(torch.nn.Module):
    def __init__(self, model, base_data):
        super().__init__()
        self.model = model
        self.base_data = base_data
    
    def forward(self, x, edge_index, **kwargs):
        data = self.base_data.clone()
        data.x = x
        data.edge_index = edge_index
        
        # í•„ìˆ˜ ì†ì„±ë“¤ ë³µì‚¬
        data.x_vision = self.base_data.x_vision
        data.x_audio = self.base_data.x_audio
        data.vision_lengths = self.base_data.vision_lengths
        data.audio_lengths = self.base_data.audio_lengths
        data.node_types = self.base_data.node_types
        
        # ptr ì†ì„± ì¶”ê°€ (ë‹¨ì¼ ê·¸ë˜í”„ìš©)
        if not hasattr(data, 'ptr'):
            data.ptr = torch.tensor([0, data.x.size(0)], dtype=torch.long, device=x.device)
        
        # batch ì†ì„± í™•ì¸
        if not hasattr(data, 'batch'):
            data.batch = torch.zeros(data.x.size(0), dtype=torch.long, device=x.device)
        
        return self.model(data, explanation=False)


def explain_with_gnn_explainer_verbose(model, data, target_topic, all_topics, epochs=100):
    """
    ì§„í–‰ ìƒí™©ì„ ìì„¸íˆ ì¶œë ¥í•˜ëŠ” GNN Explainer
    
    Args:
        model: í•™ìŠµëœ GNN ëª¨ë¸
        data: PyG Data ê°ì²´
        target_topic: íƒ€ê²Ÿ í† í”½ ë…¸ë“œ ID
        all_topics: ëª¨ë“  í† í”½ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
        epochs: Explainer í•™ìŠµ ì—í­ ìˆ˜
    """
    device = data.x.device
    model.eval()
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Target Topic: {target_topic}")
    print(f"ğŸ“Š Graph Info: {data.num_nodes} nodes, {data.num_edges} edges")
    print('='*70)
    
    # ë°°ì¹˜ ì •ë³´ ì¶”ê°€
    if not hasattr(data, 'batch'):
        data.batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)
    
    # ptr ì†ì„± ì¶”ê°€ (ì¤‘ìš”!)
    if not hasattr(data, 'ptr'):
        data.ptr = torch.tensor([0, data.x.size(0)], dtype=torch.long, device=device)
        print("âœ… Added 'ptr' attribute for single graph")
    
    # ëª¨ë¸ ë˜í¼ ìƒì„±
    wrapped_model = ModelWrapper(model, data).to(device)
    
    try:
        start_time = time.time()
        
        # Progress GNN Explainer ì‚¬ìš©
        explainer = Explainer(
            model=wrapped_model,
            algorithm=ProgressGNNExplainer(epochs=epochs),
            explanation_type='model',
            node_mask_type='attributes',
            edge_mask_type='object',
            model_config=dict(
                mode='binary_classification',
                task_level='graph',
                return_type='raw',
            ),
        )
        
        # ì„¤ëª… ìƒì„±
        print("\nâ³ Generating explanation...")
        explanation = explainer(data.x, data.edge_index, batch=data.batch)
        
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  Total Time: {elapsed:.2f} seconds")
        
        # ê²°ê³¼ ìš”ì•½
        node_mask = explanation.node_mask.sum(dim=1).detach().cpu().numpy()
        edge_mask = explanation.edge_mask.detach().cpu().numpy()
        
        print("\nğŸ“ˆ Explanation Statistics:")
        print(f"  Node importance - Mean: {node_mask.mean():.4f}, Std: {node_mask.std():.4f}")
        print(f"  Edge importance - Mean: {edge_mask.mean():.4f}, Std: {edge_mask.std():.4f}")
        print(f"  Top node importance: {node_mask.max():.4f}")
        print(f"  Top edge importance: {edge_mask.max():.4f}")
        
        return explanation, explainer.algorithm.loss_history
        
    except Exception as e:
        print(f"\nâŒ GNNExplainer failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def plot_loss_curve(loss_history):
    """GNN Explainerì˜ ì†ì‹¤ ê³¡ì„  ì‹œê°í™”"""
    if loss_history is None or len(loss_history) == 0:
        return
    
    plt.figure(figsize=(10, 5))
    plt.plot(loss_history, linewidth=2, color='#4ECDC4')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('GNN Explainer Training Loss', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def compare_all_methods(model, data, target_topic, all_topics):
    """
    ëª¨ë“  ì„¤ëª… ë°©ë²•ì„ ë¹„êµí•˜ê³  ì‹œê°„ ì¸¡ì •
    """
    device = data.x.device
    model.eval()
    
    results = {}
    timings = {}
    
    print(f"\n{'='*70}")
    print("ğŸ”¬ COMPARING ALL EXPLANATION METHODS")
    print('='*70)
    
    # 1. SIMPLE
    print("\n[1/4] ğŸš€ SIMPLE (Input Feature Similarity)")
    start = time.time()
    node_attr, edge_attr = explain_simple(model, data, target_topic)
    timings['simple'] = time.time() - start
    results['simple'] = (node_attr, edge_attr, "SIMPLE")
    print(f"  âœ… Completed in {timings['simple']:.3f}s")
    
    # 2. COSINE
    print("\n[2/4] ğŸ§® COSINE (Model Embedding Similarity)")
    start = time.time()
    node_attr, edge_attr, _ = explain_with_model_embeddings(model, data, target_topic)
    timings['cosine'] = time.time() - start
    results['cosine'] = (node_attr, edge_attr, "COSINE")
    print(f"  âœ… Completed in {timings['cosine']:.3f}s")
    
    # 3. GRADIENT
    print("\n[3/4] ğŸ“‰ GRADIENT (Captum IntegratedGradients)")
    start = time.time()
    try:
        node_attr, edge_attr, _ = explain_with_gradients(model, data, target_topic)
        timings['gradient'] = time.time() - start
        results['gradient'] = (node_attr, edge_attr, "GRADIENT")
        print(f"  âœ… Completed in {timings['gradient']:.3f}s")
    except Exception as e:
        print(f"  âŒ Failed: {e}")
        timings['gradient'] = None
    
    # 4. GNN_EXPLAINER
    print("\n[4/4] ğŸ¯ GNN_EXPLAINER (Structure + Features)")
    start = time.time()
    explanation, loss_history = explain_with_gnn_explainer_verbose(
        model, data, target_topic, all_topics, epochs=100
    )
    timings['gnn'] = time.time() - start
    
    if explanation is not None:
        node_attr = explanation.node_mask.sum(dim=1).detach().cpu().numpy()
        edge_attr = explanation.edge_mask.detach().cpu().numpy()
        results['gnn'] = (node_attr, edge_attr, "GNN_EXPLAINER")
        
        # ì†ì‹¤ ê³¡ì„  ì¶œë ¥
        plot_loss_curve(loss_history)
    
    # ì‹œê°„ ë¹„êµ
    print(f"\n{'='*70}")
    print("â±ï¸  EXECUTION TIME COMPARISON")
    print('='*70)
    for method, t in timings.items():
        if t is not None:
            print(f"  {method.upper():15s}: {t:6.3f}s")
    print('='*70)
    
    return results, timings


def explain_simple(model, data, target_topic):
    """ê°„ë‹¨í•œ ì…ë ¥ í”¼ì²˜ ê¸°ë°˜ ì„¤ëª…"""
    target_feature = data.x[target_topic].unsqueeze(0)
    node_attr = F.cosine_similarity(target_feature, data.x).cpu().numpy()
    
    edge_index = data.edge_index
    src_sim = node_attr[edge_index[0].cpu()]
    dst_sim = node_attr[edge_index[1].cpu()]
    edge_attr = (src_sim + dst_sim) / 2
    
    return node_attr, edge_attr


def explain_with_model_embeddings(model, data, target_topic):
    """ëª¨ë¸ ì„ë² ë”© ê¸°ë°˜ ì„¤ëª…"""
    device = data.x.device
    model.eval()
    
    with torch.no_grad():
        x, edge_index = data.x, data.edge_index
        node_types = data.node_types
        
        # Text projection
        if hasattr(model, 'use_text_proj') and model.use_text_proj:
            x = model.text_proj(x)
        x = model.dropout_text(x)
        
        # Vision/Audio ì²˜ë¦¬ (GAT.pyì™€ ë™ì¼í•˜ê²Œ)
        flat_node_types = []
        if isinstance(node_types[0], list):
            for sublist in node_types: 
                flat_node_types.extend(sublist)
        else:
            flat_node_types = node_types
        
        vision_indices = [i for i, t in enumerate(flat_node_types) if t == 'vision']
        audio_indices = [i for i, t in enumerate(flat_node_types) if t == 'audio']
        
        # Vision LSTM
        if data.x_vision.size(0) > 0 and len(vision_indices) > 0:
            h_vision, _ = model.vision_lstm(data.x_vision, data.vision_lengths)
            if len(vision_indices) == h_vision.size(0):
                x[vision_indices] = h_vision.to(x.dtype)
        
        # Audio LSTM
        if data.x_audio.size(0) > 0 and len(audio_indices) > 0:
            h_audio, _ = model.audio_lstm(data.x_audio, data.audio_lengths)
            if len(audio_indices) == h_audio.size(0):
                x[audio_indices] = h_audio.to(x.dtype)
        
        # GAT layers (num_layersì— ë”°ë¼)
        x = F.dropout(x, p=model.dropout_g, training=False)
        x = model.conv1(x, edge_index)
        x = model.norm1(x)
        x = F.elu(x)
        
        if model.num_layers >= 3:
            x_in = x
            x = F.dropout(x, p=model.dropout_g, training=False)
            x = model.conv2(x, edge_index)
            x = model.norm2(x + x_in) if hasattr(model, 'norm2') else x
            x = F.elu(x)
        
        if model.num_layers >= 4:
            x_in = x
            x = F.dropout(x, p=model.dropout_g, training=False)
            x = model.conv3(x, edge_index)
            x = model.norm3(x + x_in) if hasattr(model, 'norm3') else x
            x = F.elu(x)
        
        x = F.dropout(x, p=model.dropout_g, training=False)
        x = model.conv4(x, edge_index)
        x = model.norm4(x)
        
        target_emb = x[target_topic].unsqueeze(0)
        node_attr = F.cosine_similarity(target_emb, x).cpu().numpy()
    
    edge_index = data.edge_index
    src_sim = node_attr[edge_index[0].cpu()]
    dst_sim = node_attr[edge_index[1].cpu()]
    edge_attr = (src_sim + dst_sim) / 2
    
    return node_attr, edge_attr, "Model Embedding"


def explain_with_gradients(model, data, target_topic):
    """Gradient ê¸°ë°˜ ì„¤ëª…"""
    device = data.x.device
    model.eval()
    
    def forward_func(node_features):
        data_copy = data.clone()
        data_copy.x = node_features
        
        # í•„ìˆ˜ ì†ì„± ë³µì‚¬
        data_copy.x_vision = data.x_vision
        data_copy.x_audio = data.x_audio
        data_copy.vision_lengths = data.vision_lengths
        data_copy.audio_lengths = data.audio_lengths
        data_copy.node_types = data.node_types
        
        # batchì™€ ptr ì¶”ê°€
        if not hasattr(data_copy, 'batch'):
            data_copy.batch = torch.zeros(data_copy.x.size(0), dtype=torch.long, device=device)
        if not hasattr(data_copy, 'ptr'):
            data_copy.ptr = torch.tensor([0, data_copy.x.size(0)], dtype=torch.long, device=device)
        
        out = model(data_copy, explanation=False)
        if out.dim() == 1:
            out = out.unsqueeze(-1)
        return out

    ig = IntegratedGradients(forward_func)
    baseline_x = torch.zeros_like(data.x)
    
    attributions = ig.attribute(
        data.x,
        baselines=baseline_x,
        target=0,
        n_steps=50,
        internal_batch_size=1
    )
    
    node_attr = attributions.abs().sum(dim=1).cpu().detach().numpy()
    target_importance = node_attr[target_topic]
    node_attr = node_attr / (target_importance + 1e-8)
    
    edge_index = data.edge_index
    src_imp = node_attr[edge_index[0].cpu()]
    dst_imp = node_attr[edge_index[1].cpu()]
    edge_attr = (src_imp + dst_imp) / 2
    
    return node_attr, edge_attr, "Gradient"

def visualize_topic_subgraph(data, node_attr, edge_attr, target_topic_idx, title="Topic Subgraph Visualization"):
    """
    íŠ¹ì • í† í”½ê³¼ ì—°ê²°ëœ í•˜ìœ„ ë…¸ë“œë“¤(Text -> Proxy -> Vision/Audio)ë§Œ ì¶”ì¶œí•˜ì—¬
    ê³„ì¸µì ìœ¼ë¡œ ì‹œê°í™”í•˜ê³ , ë…¸ë“œ IDì™€ ì¤‘ìš”ë„ ìˆ˜ì¹˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    # ---------------------------------------------------------
    # 1. ì„œë¸Œê·¸ë˜í”„ ë…¸ë“œ ë° ì—£ì§€ í•„í„°ë§ (ê³„ì¸µì  íƒìƒ‰)
    # ---------------------------------------------------------
    edge_index = data.edge_index.cpu().numpy()
    src, dst = edge_index[0], edge_index[1]
    
    # ë…¸ë“œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”
    flat_node_types = []
    if isinstance(data.node_types[0], list):
        for sublist in data.node_types: flat_node_types.extend(sublist)
    else:
        flat_node_types = data.node_types
    flat_node_types = np.array(flat_node_types)

    # (1) Topic ë…¸ë“œ (Layer 0)
    nodes_layer_0 = [target_topic_idx]
    
    # (2) Connected Transcription ë…¸ë“œ ì°¾ê¸° (Layer 1)
    # Topicê³¼ ì—°ê²°ëœ ì—£ì§€ ì¤‘, ìƒëŒ€ë°©ì´ transcriptionì¸ ê²ƒ
    connected_edges_0 = np.where((src == target_topic_idx) | (dst == target_topic_idx))[0]
    nodes_layer_1 = []
    for idx in connected_edges_0:
        u, v = src[idx], dst[idx]
        neighbor = v if u == target_topic_idx else u
        if flat_node_types[neighbor] == 'transcription':
            nodes_layer_1.append(neighbor)
    nodes_layer_1 = list(set(nodes_layer_1))

    # (3) Connected Proxy ë…¸ë“œ ì°¾ê¸° (Layer 2)
    # Layer 1(Text) ë…¸ë“œë“¤ê³¼ ì—°ê²°ëœ Proxy ì°¾ê¸°
    nodes_layer_2 = []
    if nodes_layer_1:
        connected_edges_1 = np.isin(src, nodes_layer_1) | np.isin(dst, nodes_layer_1)
        edge_indices_1 = np.where(connected_edges_1)[0]
        for idx in edge_indices_1:
            u, v = src[idx], dst[idx]
            # uê°€ Textë©´ vê°€ ì´ì›ƒ, vê°€ Textë©´ uê°€ ì´ì›ƒ
            neighbor = v if u in nodes_layer_1 else u
            if flat_node_types[neighbor] == 'proxy':
                nodes_layer_2.append(neighbor)
    nodes_layer_2 = list(set(nodes_layer_2))

    # (4) Connected Vision/Audio ë…¸ë“œ ì°¾ê¸° (Layer 3)
    nodes_layer_3 = []
    if nodes_layer_2:
        connected_edges_2 = np.isin(src, nodes_layer_2) | np.isin(dst, nodes_layer_2)
        edge_indices_2 = np.where(connected_edges_2)[0]
        for idx in edge_indices_2:
            u, v = src[idx], dst[idx]
            neighbor = v if u in nodes_layer_2 else u
            if flat_node_types[neighbor] in ['vision', 'audio']:
                nodes_layer_3.append(neighbor)
    nodes_layer_3 = list(set(nodes_layer_3))

    # ì „ì²´ ì„œë¸Œê·¸ë˜í”„ ë…¸ë“œ ì§‘í•©
    all_subgraph_nodes = set(nodes_layer_0 + nodes_layer_1 + nodes_layer_2 + nodes_layer_3)

    # ---------------------------------------------------------
    # 2. NetworkX ê·¸ë˜í”„ ìƒì„± ë° ì†ì„± í• ë‹¹
    # ---------------------------------------------------------
    G = nx.DiGraph()
    
    color_map = {
        'summary': '#FF6B6B', 'topic': '#4ECDC4', 'transcription': '#45B7D1',
        'proxy': '#A0A0A0', 'vision': '#FFA07A', 'audio': '#98D8C8'
    }

    # ë…¸ë“œ ì¶”ê°€
    for node_idx in all_subgraph_nodes:
        n_type = flat_node_types[node_idx]
        
        # ì¤‘ìš”ë„ ì ìˆ˜ (node_attrì—ì„œ ê°€ì ¸ì˜´)
        score = node_attr[node_idx] if node_attr is not None else 0.0
        
        # ë ˆì´ì–´ ì •ë³´ í• ë‹¹ (ì‹œê°í™” ë°°ì¹˜ìš©)
        if node_idx in nodes_layer_0: layer = 0
        elif node_idx in nodes_layer_1: layer = 1
        elif node_idx in nodes_layer_2: layer = 2
        else: layer = 3
        
        # ë¼ë²¨ í¬ë§·: "ID\n(0.xx)"
        label_text = f"{node_idx}\n({score:.3f})"
        
        G.add_node(node_idx, 
                   color=color_map.get(n_type, 'gray'), # type: ignore
                   layer=layer,
                   label=label_text,
                   size=3000 if layer==0 else 1500)

    # ì—£ì§€ ì¶”ê°€ (ì„œë¸Œê·¸ë˜í”„ ë…¸ë“œë¼ë¦¬ì˜ ì—°ê²°ë§Œ)
    final_edges = []
    final_edge_colors = []
    final_edge_widths = []
    
    for i in range(len(src)):
        u, v = src[i], dst[i]
        if u in all_subgraph_nodes and v in all_subgraph_nodes:
            # ë°©í–¥ì„± ì •ë¦¬ (ê³„ì¸µ ìœ„ -> ì•„ë˜)
            # Layerê°€ ì‘ì€ ìª½ì—ì„œ í° ìª½ìœ¼ë¡œ í™”ì‚´í‘œ
            layer_u = G.nodes[u]['layer']
            layer_v = G.nodes[v]['layer']
            
            # ê°™ì€ ë ˆì´ì–´ë¼ë¦¬ëŠ” ì—°ê²° ì•ˆ í•¨ (ê¹”ë”í•¨ì„ ìœ„í•´)
            if layer_u == layer_v: continue
            
            # ìœ„ì—ì„œ ì•„ë˜ë¡œ ê·¸ë¦¬ë„ë¡ source/target ì¡°ì •
            source, target = (u, v) if layer_u < layer_v else (v, u)
            
            # ì¤‘ë³µ ì—£ì§€ ë°©ì§€
            if not G.has_edge(source, target):
                imp = edge_attr[i] if edge_attr is not None else 0.5
                
                # ì—£ì§€ ìŠ¤íƒ€ì¼ ê³„ì‚°
                width = 1 + 5 * imp  # ì¤‘ìš”í• ìˆ˜ë¡ êµµê²Œ
                alpha = max(0.2, imp) # ì¤‘ìš”í• ìˆ˜ë¡ ì§„í•˜ê²Œ
                
                G.add_edge(source, target)
                final_edges.append((source, target))
                final_edge_colors.append((0.5, 0.5, 0.5, alpha)) # RGBA Gray
                final_edge_widths.append(width)

    # ---------------------------------------------------------
    # 3. ì‹œê°í™” (Multipartite Layout - ê³„ì¸µí˜•)
    # ---------------------------------------------------------
    plt.figure(figsize=(14, 10))
    
    # ê³„ì¸µí˜• ë ˆì´ì•„ì›ƒ (Multipartite)
    pos = nx.multipartite_layout(G, subset_key="layer", align='horizontal')
    # ë°©í–¥ì„ ìœ„(Topic) -> ì•„ë˜(Audio)ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ yì¶• ë°˜ì „ ì¡°ì •ì€ multipartiteê°€ ìë™ìœ¼ë¡œ í•´ì¤Œ
    # (ê¸°ë³¸ì ìœ¼ë¡œ layer 0ì´ ì™¼ìª½ì´ë‚˜ ìœ„ìª½ìœ¼ë¡œ ê°)
    
    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    colors = [G.nodes[n]['color'] for n in G.nodes]
    sizes = [G.nodes[n]['size'] for n in G.nodes]
    
    # ë…¸ë“œ ë³¸ì²´
    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=sizes, edgecolors='black')
    
    # ë…¸ë“œ ë¼ë²¨ (ID + ì ìˆ˜)
    labels = nx.get_node_attributes(G, 'label')
    nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_family='Malgun Gothic', font_weight='bold')
    
    # ì—£ì§€ ê·¸ë¦¬ê¸°
    for i, edge in enumerate(final_edges):
        nx.draw_networkx_edges(G, pos, 
                               edgelist=[edge], 
                               width=final_edge_widths[i], 
                               edge_color=[final_edge_colors[i]],
                               arrowstyle='-', arrowsize=20)

    # ë²”ë¡€ ë° íƒ€ì´í‹€
    legend_elements = [mpatches.Patch(color=c, label=l) for l, c in color_map.items()]
    plt.legend(handles=legend_elements, loc='lower right')
    
    plt.title(title, fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# =============================================================================
# [ìµœì¢… ì‹¤í–‰ ì½”ë“œ] ëª¨ë“  ë°©ë²• ì‹¤í–‰ ë° "í† í”½ ì¤‘ì‹¬ ì„œë¸Œê·¸ë˜í”„" ì‹œê°í™”
# =============================================================================

# 1. íƒ€ê²Ÿ ê·¸ë˜í”„ ë° í† í”½ ì„¤ì •
target_graph = graphs[0].to(device)

# 'topic' íƒ€ì…ì„ ê°€ì§„ ë…¸ë“œë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
all_topic_indices = [i for i, t in enumerate(target_graph.node_types) if t == 'topic']
target_topic_idx = all_topic_indices[1] # 2ë²ˆì§¸ í† í”½ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •

logger.info(f"Target Graph: {target_graph.num_nodes} nodes, {target_graph.num_edges} edges")
logger.info(f"Target Topic Node Index: {target_topic_idx}")

# 2. ëª¨ë“  ì„¤ëª… ë°©ë²• ì‹¤í–‰ (ê²°ê³¼ ê³„ì‚°)
logger.info("Comparing all methods...")
results, timings = compare_all_methods(
   model=model,
   data=target_graph,
   target_topic=target_topic_idx,
   all_topics=all_topic_indices
)

print("\n" + "="*60)
print("ğŸ¨ STARTING TOPIC-CENTERED VISUALIZATION FOR ALL METHODS")
print("="*60)

# 3. ê²°ê³¼ ë°˜ë³µë¬¸ -> "visualize_topic_subgraph" í•¨ìˆ˜ í˜¸ì¶œ
for method_name, (node_attr, edge_attr, desc) in results.items():
    print(f"\nâ–¶ Method: {desc}")
    
    # (1) ìˆ˜ì¹˜ì  ì¤‘ìš”ë„ ìš”ì•½ ì¶œë ¥ (Vision/Audio)
    vision_indices = [i for i, t in enumerate(target_graph.node_types) if t == 'vision']
    audio_indices = [i for i, t in enumerate(target_graph.node_types) if t == 'audio']
    
    if vision_indices:
        v_imp = node_attr[vision_indices]
        print(f"  ğŸ“· Vision Importance | Avg: {v_imp.mean():.4f}, Max: {v_imp.max():.4f}")
    else:
        print("  ğŸ“· Vision Importance | No Vision Nodes")
        
    if audio_indices:
        a_imp = node_attr[audio_indices]
        print(f"  ğŸ¤ Audio Importance  | Avg: {a_imp.mean():.4f}, Max: {a_imp.max():.4f}")
    else:
        print("  ğŸ¤ Audio Importance  | No Audio Nodes")

    # (2) [í•µì‹¬ ìˆ˜ì •] ì¤‘ìš”ë„ ì •ê·œí™” (0~1) -> ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
    # ì‹œê°í™” í•¨ìˆ˜ê°€ ìƒ‰ìƒ/êµµê¸°ë¥¼ ì˜ í‘œí˜„í•˜ë„ë¡ Min-Max ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    if node_attr.max() != node_attr.min():
        norm_node_attr = (node_attr - node_attr.min()) / (node_attr.max() - node_attr.min() + 1e-9)
    else:
        norm_node_attr = node_attr

    if edge_attr is not None and edge_attr.max() != edge_attr.min():
        norm_edge_attr = (edge_attr - edge_attr.min()) / (edge_attr.max() - edge_attr.min() + 1e-9)
    else:
        norm_edge_attr = edge_attr

    # ê³„ì¸µí˜• ì„œë¸Œê·¸ë˜í”„ ì‹œê°í™” í˜¸ì¶œ
    visualize_topic_subgraph(
        data=target_graph,
        node_attr=norm_node_attr,
        edge_attr=norm_edge_attr,
        target_topic_idx=target_topic_idx,
        title=f"Topic {target_topic_idx} Subgraph Explanation ({desc})"
    )
    
    print("-" * 60)

print("\nâœ… All visualizations completed.")